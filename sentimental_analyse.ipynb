{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12421d34-5d66-4d2c-a403-17d01bd1b4ec",
   "metadata": {},
   "source": [
    "## Importing Essential Libraries\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1cde424-2030-4aab-8d7a-c7374588e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78796569-f9b4-440f-abdc-0ea686178ffc",
   "metadata": {},
   "source": [
    "## Function to Download and Read Labeled Sentences\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0de67d5f-ccb6-4799-b6fb-89af18b7a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    local_file = local_file.replace('%20', ' ')\n",
    "    p = tf.keras.utils.get_file(local_file, url, extract=True, cache_dir = '.')\n",
    "    local_folder = os.path.join('datasets', local_file.split('.')[0])\n",
    "    labeled_sentences = []\n",
    "    for labeled_filename in os.listdir(local_folder):\n",
    "        if labeled_filename.endswith('labelled.txt'):\n",
    "            with open(os.path.join(\n",
    "                local_folder, labeled_filename), 'r') as f:\n",
    "                for line in f:\n",
    "                    sentence, label = line.strip().split('\\t')\n",
    "                    labeled_sentences.append((sentence, label))\n",
    "    return labeled_sentences\n",
    "labeled_sentences = download_and_read(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/\" + \n",
    "    \"00331/sentiment%20labelled%20sentences.zip\") \n",
    "sentence = [s for (s, l) in labeled_sentences]\n",
    "labels = [int(l) for (s,l) in labeled_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032a73d-d2f0-4948-a7bf-38713c337b42",
   "metadata": {},
   "source": [
    "## Text Tokenization and Vocabulary Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c1cd940-6ab0-4c8b-a81e-b85ffea50db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vacab size: 5271\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentence)\n",
    "vocab_size = len(tokenizer.word_counts)\n",
    "print('vacab size: {:d}'.format(vocab_size))\n",
    "word2inx = tokenizer.word_index\n",
    "ind2word = {v:k for (k,v) in word2inx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ddd042-710b-4b7a-be08-16a66d4627f7",
   "metadata": {},
   "source": [
    "## Analyzing Sentence Length Distribution\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69ece19a-6c8a-4116-8cac-e69dc23e737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(75, 16.0), (80, 18.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)]\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = np.array([len(s.split()) for s in sentence])\n",
    "print([(p, np.percentile(seq_lengths, p)) for p in [75,80, 90,95,99,100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af92e7-728f-4065-a33e-1d89df9f1af0",
   "metadata": {},
   "source": [
    "## Preparing Dataset with Padded Sequences\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea219c47-474d-43ef-a2ce-ba94a4f92f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = 64\n",
    "#create dataset\n",
    "sentences_as_int = tokenizer.texts_to_sequences(sentence)\n",
    "sentences_as_int = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sentences_as_int, maxlen = max_seqlen)\n",
    "labels_as_int = np.array(labels)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sentences_as_int, labels_as_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a54c8d-cc28-4458-bd51-cf1c67aa0adf",
   "metadata": {},
   "source": [
    "## Splitting Data into Training, Validation, and Test Sets\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adc6270d-ace6-41c0-833d-73e838642a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shuffle(10000)\n",
    "test_size = len(sentence) // 3\n",
    "val_size = (len(sentence) - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "batch_size = 64\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdda7fa-a815-40a5-bae3-1201667ec295",
   "metadata": {},
   "source": [
    "## Defining, Compiling, and Training the Sentiment Analysis Model\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10376d43-2d71-4773-a7a5-88484690ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sentimental_analysis_model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     multiple                  337408    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  multiple                 66048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 411,777\n",
      "Trainable params: 411,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 8s 37ms/step - loss: 0.6955 - accuracy: 0.4767 - val_loss: 0.6952 - val_accuracy: 0.4050\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.6399 - accuracy: 0.6922 - val_loss: 0.8281 - val_accuracy: 0.5950\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.4492 - accuracy: 0.8222 - val_loss: 0.5851 - val_accuracy: 0.7350\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.2837 - accuracy: 0.9039 - val_loss: 0.7509 - val_accuracy: 0.6400\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.1607 - accuracy: 0.9500 - val_loss: 0.9271 - val_accuracy: 0.6050\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0874 - accuracy: 0.9767 - val_loss: 0.7756 - val_accuracy: 0.7400\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0481 - accuracy: 0.9883 - val_loss: 0.8019 - val_accuracy: 0.7750\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0325 - accuracy: 0.9939 - val_loss: 0.8859 - val_accuracy: 0.7300\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0229 - accuracy: 0.9967 - val_loss: 0.9664 - val_accuracy: 0.7300\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0174 - accuracy: 0.9978 - val_loss: 0.9741 - val_accuracy: 0.7350\n"
     ]
    }
   ],
   "source": [
    "class SentimentalAnalysisModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, max_seqlen, **kwargs):\n",
    "        super(SentimentalAnalysisModel, self).__init__(**kwargs)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, max_seqlen)\n",
    "        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(max_seqlen))\n",
    "        self.dense = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.bilstm(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "model = SentimentalAnalysisModel(vocab_size+1, max_seqlen)\n",
    "model.build(input_shape=(batch_size, max_seqlen))\n",
    "model.summary()\n",
    "\n",
    "#compile\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "#train\n",
    "data_dir = './data'\n",
    "logs_dir = os.path.join('./logs')\n",
    "best_model_file = os.path.join(data_dir, 'best_model.h5')\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file,\n",
    "                                                save_weights_only=True,\n",
    "                                                save_best_only=  True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir = logs_dir)\n",
    "num_epochs = 10\n",
    "history = model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset,\n",
    "                    callbacks = [checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68437f-bb9d-4b2d-86d6-2136971639af",
   "metadata": {},
   "source": [
    "## Loading and Evaluating the Trained Model\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "620af578-7aee-421a-aa37-bc2b3c1cff11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 16ms/step - loss: 0.5287 - accuracy: 0.7450\n",
      "test loss: 0.528709, test accuracy: 0.745000\n"
     ]
    }
   ],
   "source": [
    "best_model = SentimentalAnalysisModel(vocab_size+1, max_seqlen)\n",
    "best_model.build(input_shape=(batch_size, max_seqlen))\n",
    "best_model.load_weights(best_model_file)\n",
    "best_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "test_loss, test_acc = best_model.evaluate(test_dataset)\n",
    "print('test loss: {:3f}, test accuracy: {:3f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581d1b7e-9e70-4dba-844d-6e91d9554373",
   "metadata": {},
   "source": [
    "## Generating Predictions and Evaluating the Model on Test Data\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb3ce61c-0d12-4d89-b1a0-30c07e9f9145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tso there is no way for me to plug it in here in the us unless i go by a converter\n",
      "1\t1\tgood case excellent value\n",
      "1\t1\tgreat for the jawbone\n",
      "0\t0\ttied to charger for conversations lasting more than 45 minutes major problems\n",
      "1\t1\tthe mic is great\n",
      "0\t0\ti have to jiggle the plug to get it to line up right to get decent volume\n",
      "0\t0\tif you have several dozen or several hundred contacts then imagine the fun of sending each of them one by one\n",
      "1\t1\tif you are razr owner you must have this\n",
      "0\t0\tneedless to say i wasted my money\n",
      "0\t0\twhat a waste of money and time\n",
      "1\t1\tand the sound quality is great\n",
      "1\t0\the was very impressed when going from the original battery to the extended battery\n",
      "0\t0\tif the two were seperated by a mere 5 ft i started to notice excessive static and garbled sound from the headset\n",
      "1\t1\tvery good quality though\n",
      "0\t0\tthe design is very odd as the ear clip is not very comfortable at all\n",
      "1\t1\thighly recommend for any one who has a blue tooth phone\n",
      "0\t0\ti advise everyone do not be fooled\n",
      "1\t1\tso far so good\n",
      "1\t1\tworks great\n",
      "0\t0\tit clicks into place in a way that makes you wonder how long that mechanism would last\n",
      "0\t0\ti went on motorola's website and followed all directions but could not get it to pair again\n",
      "1\t1\ti bought this to use with my kindle fire and absolutely loved it\n",
      "0\t1\tthe commercials are the most misleading\n",
      "1\t1\ti have yet to run this new battery below two bars and that's three days without charging\n",
      "0\t1\ti bought it for my mother and she had a problem with the battery\n",
      "1\t1\tgreat pocket pc phone combination\n",
      "1\t1\ti've owned this phone for 7 months now and can say that it's the best mobile phone i've had\n",
      "0\t0\ti didn't think that the instructions provided were helpful to me\n",
      "0\t1\tpeople couldnt hear me talk and i had to pull out the earphone and talk on the phone\n",
      "0\t0\tdoesn't hold charge\n",
      "0\t0\tthis is a simple little phone to use but the breakage is unacceptible\n",
      "1\t0\tthis product is ideal for people like me whose ears are very sensitive\n",
      "0\t0\tit is unusable in a moving car at freeway speed\n",
      "0\t1\ti have two more years left in this contract and i hate this phone\n",
      "1\t0\tcar charger as well as ac charger are included to make sure you never run out of juice highy recommended\n",
      "0\t1\tyou need at least 3 mins to get to your phone book from the time you first turn on the phone battery life is short\n",
      "1\t1\tit has kept up very well\n",
      "0\t0\tpoor talk time performance\n",
      "1\t1\tthe case is great and works fine with the 680\n",
      "0\t0\tworthless product\n",
      "1\t1\tit has a great camera thats 2mp and the pics are nice and clear with great picture quality\n",
      "0\t0\ti was not impressed by this product\n",
      "1\t1\tnice headset priced right\n",
      "0\t0\ti only hear garbage for audio\n",
      "1\t1\texcellent bluetooth headset\n",
      "1\t1\tit has all the features i want\n",
      "0\t1\twho in their right mind is gonna buy this battery\n",
      "0\t0\tafter arguing with verizon regarding the dropped calls we returned the phones after two days\n",
      "1\t1\tthis case seems well made\n",
      "0\t0\tdisappointed with battery\n",
      "0\t0\tnot loud enough and doesn't turn on like it should\n",
      "1\t0\tgood protection and does not make phone too bulky\n",
      "1\t0\ta usable keyboard actually turns a pda into a real world useful machine instead of just a neat gadget\n",
      "1\t0\tthis phone is pretty sturdy and i've never had any large problems with it\n",
      "1\t1\ti love this thing\n",
      "1\t1\teverything about it is fine and reasonable for the price i e\n",
      "0\t0\tvery disappointed\n",
      "1\t1\ti even dropped this phone into a stream and it was submerged for 15 seconds and it still works great\n",
      "1\t0\ti have been very happy with the 510 and have had no complaints from any one regarding my sound quality on their end\n",
      "0\t0\tthe buttons for on and off are bad\n",
      "0\t1\tessentially you can forget microsoft's tech support\n",
      "1\t1\ti really recommend this faceplates since it looks very nice elegant and cool\n",
      "1\t0\tthese headphones were a great find and i think they are perhaps the best purchase i've made in the last several years seriously\n",
      "0\t0\tbuy a different phone but not this\n",
      "accuracy score: 0.745000\n",
      "confusion matrix\n",
      "[[441  59]\n",
      " [196 304]]\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = [], []\n",
    "ind2word[0] = 'PAD'\n",
    "is_first_batch = True\n",
    "total_processed = 0  # Добавляем счетчик для общего количества обработанных примеров\n",
    "\n",
    "for test_batch in test_dataset:\n",
    "    input_b, labels_b = test_batch\n",
    "    pred_batch = best_model.predict(input_b)\n",
    "    predictions.extend([(1 if p > .5 else 0) for p in pred_batch])\n",
    "    labels.extend([l for l in labels_b])\n",
    "    if is_first_batch:\n",
    "        for rid in range(input_b.shape[0]):\n",
    "            words = [ind2word[idx] for idx in input_b[rid].numpy()]\n",
    "            words = [w for w in words if w != 'PAD']\n",
    "            sentence = ' '.join(words)\n",
    "            # Используем total_processed + rid для доступа к правильному индексу\n",
    "            print('{:d}\\t{:d}\\t{:s}'.format(labels[total_processed + rid], predictions[total_processed + rid], sentence))\n",
    "        is_first_batch = False\n",
    "    total_processed += input_b.shape[0]  # Обновляем счетчик после обработки пакета\n",
    "\n",
    "print('accuracy score: {:3f}'.format(accuracy_score(labels, predictions)))\n",
    "print('confusion matrix')\n",
    "print(confusion_matrix(labels, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
